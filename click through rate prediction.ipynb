{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15d1600c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.metrics import log_loss\n",
    "import time\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c425cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.read_csv('all_dumm_val.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dbb16d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('all_dumm_test.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89c90a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('all_dumm_tt.csv.gz',  usecols=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc6ce75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, df_val[['click']]]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad4d2b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, df_test[['click']]]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7804bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.830194\n",
      "1    0.169806\n",
      "Name: click, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 83.1% values are 0 and 16.9% values are 1 \n",
    "print(df.click.value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87c39569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "586faa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = df_val['click']\n",
    "x_val = df_val.drop(columns=df.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de8449ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class for logistic regression with stochastic gradient descent\n",
    "\n",
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(LR, start, eta=0.1,panelty=None, lamb=1):\n",
    "        LR.eta = eta\n",
    "        LR.beta = start\n",
    "        LR.penalty=panelty\n",
    "        LR.lamb=lamb\n",
    "    \n",
    "    def fit(LR, X, y, eta_i):\n",
    "        n,k=X.shape\n",
    "        z = np.dot(X, LR.beta)\n",
    "        p = 1 / (1 + np.exp(-z))\n",
    "        gradient = np.dot(X.T, (p - y)) / y.size\n",
    "        \n",
    "        if LR.panelty=='l1':\n",
    "            gradient[1:k,] += LR.lamb*np.sign(LR.beta)[1:k,]/y.size\n",
    "        elif LR.panelty=='l2':\n",
    "            gradient[1:k,] += LR.lamb*LR.beta[1:k,]/y.size\n",
    "            \n",
    "        LR.beta -= eta_i * gradient\n",
    "        loss = (-y * np.log(p) - (1 - y) * np.log(1 - p)).mean()\n",
    "    \n",
    "    def predict_loss(LR, X, y):   \n",
    "        z = np.dot(X, LR.beta)\n",
    "        p = 1 / (1 + np.exp(-z))\n",
    "        return (-y * np.log(p) - (1 - y) * np.log(1 - p)).mean()\n",
    "\n",
    "    \n",
    "def log_loss_NB(naive_bayes_model, X, y):\n",
    "    p = naive_bayes_model.predict_proba(X)[:,1]\n",
    "    return (-y * np.log(p) - (1 - y) * np.log(1 - p)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35817f5",
   "metadata": {},
   "source": [
    "__How you split the dataset into an estimation and evaluation sample.__ \n",
    "\n",
    "80/20\n",
    "\n",
    "Train and test sample, 80/20 using train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a9ebd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating logistic regression model\n",
      "Chunk nr 10, avg test loss for the last 10 chunks = log-loss test 0.4601, log-loss training 0.4628\n",
      "Chunk nr 20, avg test loss for the last 10 chunks = log-loss test 0.4498, log-loss training 0.456\n",
      "Chunk nr 30, avg test loss for the last 10 chunks = log-loss test 0.445, log-loss training 0.442\n",
      "Chunk nr 40, avg test loss for the last 10 chunks = log-loss test 0.4428, log-loss training 0.4358\n",
      "Chunk nr 50, avg test loss for the last 10 chunks = log-loss test 0.4418, log-loss training 0.4366\n",
      "Chunk nr 60, avg test loss for the last 10 chunks = log-loss test 0.4413, log-loss training 0.4333\n",
      "Chunk nr 70, avg test loss for the last 10 chunks = log-loss test 0.4411, log-loss training 0.4302\n",
      "Chunk nr 80, avg test loss for the last 10 chunks = log-loss test 0.4409, log-loss training 0.427\n",
      "Chunk nr 90, avg test loss for the last 10 chunks = log-loss test 0.4408, log-loss training 0.4247\n",
      "Chunk nr 100, avg test loss for the last 10 chunks = log-loss test 0.4406, log-loss training 0.4235\n",
      "Chunk nr 110, avg test loss for the last 10 chunks = log-loss test 0.4403, log-loss training 0.4235\n",
      "Chunk nr 120, avg test loss for the last 10 chunks = log-loss test 0.4401, log-loss training 0.4238\n",
      "Chunk nr 130, avg test loss for the last 10 chunks = log-loss test 0.4399, log-loss training 0.4249\n",
      "Chunk nr 140, avg test loss for the last 10 chunks = log-loss test 0.4397, log-loss training 0.4257\n",
      "Chunk nr 150, avg test loss for the last 10 chunks = log-loss test 0.4396, log-loss training 0.4262\n",
      "Chunk nr 160, avg test loss for the last 10 chunks = log-loss test 0.4395, log-loss training 0.4263\n",
      "Chunk nr 170, avg test loss for the last 10 chunks = log-loss test 0.4395, log-loss training 0.4267\n",
      "Chunk nr 180, avg test loss for the last 10 chunks = log-loss test 0.4394, log-loss training 0.428\n",
      "Chunk nr 190, avg test loss for the last 10 chunks = log-loss test 0.4396, log-loss training 0.4305\n",
      "Chunk nr 200, avg test loss for the last 10 chunks = log-loss test 0.4401, log-loss training 0.4332\n",
      "Training stopped because log loss is no longer decreasing\n",
      "\n",
      "Algorithm needed 200 chunks to converge, and took 274.5456 seconds.\n",
      "This is an average of 1.3796 seconds per chunk\n"
     ]
    }
   ],
   "source": [
    "###### LOGISTIC REGRESSION\n",
    "\n",
    "chunksize = 10000\n",
    "df_chunk = pd.read_csv('all_dumm_tt.csv.gz', chunksize=chunksize, delimiter=',')\n",
    "\n",
    "t=1\n",
    "i=1\n",
    "avg_loss_logistic = []\n",
    "avg_loss_logistic_eval = []\n",
    "elapsed_time_logistic = []\n",
    "avg_loss_previous = 100\n",
    "avg_loss_previous_eval = 100\n",
    "count_increasing_loss = 0\n",
    "early_stopping = True    # Breaks the loop if the test performance of the model does not improve (to prevent overfitting)\n",
    "\n",
    "print('Estimating logistic regression model')\n",
    "\n",
    "for chunk in df_chunk:\n",
    "\n",
    "    start_time_chunk = time.time()\n",
    "    eta_i = 1/np.sqrt(t)\n",
    "    \n",
    "    # Select y variable from chunk, saved as series\n",
    "    y_chunk = chunk['click']\n",
    "    \n",
    "    # Remove irrelevant columns\n",
    "    chunk = chunk.drop(columns=df.columns[0], axis=1)\n",
    "    \n",
    "    # Divide the data in a training and testing set\n",
    "    #chunk_train, chunk_test, y_train, y_test = train_test_split(chunk, y_chunk, test_size=0, random_state=44)\n",
    "    chunk_train = chunk\n",
    "    y_train = y_chunk\n",
    "\n",
    "    if i == 1: \n",
    "        logistic_regression = LogisticRegression(eta=eta_i, start = np.zeros(chunk.shape[1])) # Initialize regression object\n",
    "    \n",
    "    # Update the logistic regression betas using the training data in this chunk\n",
    "    logistic_regression.fit(chunk_train,y_train, eta_i)\n",
    "    \n",
    "    # Evaluate the model performance using the testing data in this chunk\n",
    "    avg_loss_logistic.append(logistic_regression.predict_loss(x_val, y_val))\n",
    "    avg_loss_logistic_eval.append(logistic_regression.predict_loss(chunk_train, y_train))\n",
    "    \n",
    "    # Show performance after every 10 iterations\n",
    "    if i % 10 == 0:\n",
    "        print('Chunk nr {}, avg test loss for the last 10 chunks = log-loss test {}, log-loss training {}'\n",
    "              .format(i,round(np.array(avg_loss_logistic).mean(),4),round(np.array(avg_loss_logistic_eval).mean(),4)))\n",
    "        \n",
    "        # Break the loop if model did not improve with the last 10 chunks two times in a row\n",
    "        if (early_stopping == True and np.array(avg_loss_logistic).mean() > avg_loss_previous):\n",
    "            count_increasing_loss = count_increasing_loss + 1\n",
    "            if count_increasing_loss > 1:\n",
    "                print('Training stopped because log loss is no longer decreasing')\n",
    "                break\n",
    "        else:\n",
    "            count_increasing_loss = 0\n",
    "            \n",
    "        avg_loss_previous = np.array(avg_loss_logistic).mean()\n",
    "        avg_loss_logistic = []\n",
    "    \n",
    "    elapsed_time_logistic.append(time.time()-start_time_chunk)\n",
    "    \n",
    "    i = i+1\n",
    "    t = t+1\n",
    "        \n",
    "print('\\nAlgorithm needed {} chunks to converge, and took {} seconds.\\nThis is an average of {} seconds per chunk'.\n",
    "     format(i, round(np.array(elapsed_time_logistic).sum(),4), round(np.array(elapsed_time_logistic).mean(),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41f618de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating Naive Bayes model\n",
      "Chunk nr 5, avg test loss for the 5 chunks = log-loss test 0.6638, log-loss training 0.4547\n",
      "Chunk nr 10, avg test loss for the 5 chunks = log-loss test 0.6581, log-loss training 0.4517\n",
      "Chunk nr 15, avg test loss for the 5 chunks = log-loss test 0.6496, log-loss training 0.4525\n",
      "Chunk nr 20, avg test loss for the 5 chunks = log-loss test 0.6524, log-loss training 0.453\n",
      "Chunk nr 25, avg test loss for the 5 chunks = log-loss test 0.6641, log-loss training 0.4468\n",
      "Training stopped because log loss is no longer decreasing\n",
      "\n",
      "Algorithm needed 25 chunks to converge, and took 69.1073 seconds.\n",
      "This is an average of 2.8795 seconds per chunk\n"
     ]
    }
   ],
   "source": [
    "###### NAIVE BAYES\n",
    "\n",
    "df_chunk = pd.read_csv('all_dumm_tt.csv.gz', chunksize=chunksize, delimiter=',')\n",
    "\n",
    "\n",
    "i=1\n",
    "avg_loss_nb = []\n",
    "avg_loss_nb_eval = []\n",
    "elapsed_time_nb = []\n",
    "avg_loss_previous = 1\n",
    "avg_loss_previous_eval = 1\n",
    "count_increasing_loss = 0\n",
    "early_stopping = True    # Breaks the loop if the test performance of the model does not improve (to prevent overfitting)\n",
    "\n",
    "print('Estimating Naive Bayes model')\n",
    "\n",
    "for chunk in df_chunk:\n",
    "\n",
    "    start_time_chunk = time.time()\n",
    "    \n",
    "        # Select y variable from chunk, saved as series\n",
    "    y_chunk = chunk['click']\n",
    "    \n",
    "    # Remove irrelevant columns\n",
    "    chunk = chunk.drop(columns=df.columns[0], axis=1)\n",
    "    \n",
    "    # Divide the data in a training and testing set\n",
    "    #chunk_train, chunk_test, y_train, y_test = train_test_split(chunk, y_chunk, test_size=0, random_state=44)\n",
    "    chunk_train = chunk\n",
    "    y_train = y_chunk\n",
    "\n",
    "    if i == 1:         \n",
    "        naive_bayes = MultinomialNB() # Initialize naive bayes object\n",
    "\n",
    "    # Update the naive bayes using the training data in this chunk\n",
    "    naive_bayes.partial_fit(chunk_train,y_train,[0,1])\n",
    "    \n",
    "    # Evaluate the model performance using the testing data in this chunk\n",
    "    avg_loss_nb.append(log_loss(y_val,naive_bayes.predict_proba(x_val)))\n",
    "    avg_loss_nb_eval.append(log_loss(y_train,naive_bayes.predict_proba(chunk_train)))\n",
    "           \n",
    "    # Show performance after every 5 iterations\n",
    "    if i % 5 == 0:\n",
    "        print('Chunk nr {}, avg test loss for the 5 chunks = log-loss test {}, log-loss training {}'\n",
    "              .format(i,round(np.array(avg_loss_nb).mean(),4), round(np.array(avg_loss_nb_eval).mean(),4)))\n",
    "        \n",
    "        # Break the loop if model did not improve with the last 10 chunks two times in a row\n",
    "        if (early_stopping == True and np.array(avg_loss_nb).mean() > avg_loss_previous):\n",
    "            count_increasing_loss = count_increasing_loss + 1\n",
    "            if count_increasing_loss > 1:\n",
    "                print('Training stopped because log loss is no longer decreasing')\n",
    "                break\n",
    "        else:\n",
    "            count_increasing_loss = 0\n",
    "            \n",
    "        avg_loss_previous = np.array(avg_loss_nb).mean()\n",
    "        avg_loss_nb = []\n",
    "    \n",
    "    elapsed_time_nb.append(time.time()-start_time_chunk)\n",
    "    \n",
    "    # Temporary: limit training time\n",
    "    if i == 100:\n",
    "        break\n",
    "        \n",
    "    i = i+1\n",
    "        \n",
    "print('\\nAlgorithm needed {} chunks to converge, and took {} seconds.\\nThis is an average of {} seconds per chunk'.\n",
    "     format(i, round(np.array(elapsed_time_nb).sum(),4), round(np.array(elapsed_time_nb).mean(),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf3d47e",
   "metadata": {},
   "source": [
    "__How the calculation times of the two models compare to each other.__\n",
    "\n",
    "For Logistic Regression, it took 275s. For Naive Bayes, it took 69s.\n",
    "\n",
    "__How many observations each model needs for its parameters to converge.__\n",
    "\n",
    "1 chunk is 10,000.\n",
    "Logistic regression needed 200 chunks to converge so 2,000,000 obs needed.<br>\n",
    "Naive bayes needed 25 chunks to converge so 250,000 obs needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6f63c5",
   "metadata": {},
   "source": [
    "__What the prediction accuracy is of the two models based on Mean Log Loss for both the \n",
    "estimation  and  evaluation  sample.  Also  compare  with  a  model  that  simply  predicts  the \n",
    "average CTR of the estimation sample as click probability for each observation.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd14c00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss for the test set of the average prediction CTR is:  0.4555148712958835\n"
     ]
    }
   ],
   "source": [
    "# Log loss if we always predict the average CTR\n",
    "print('The loss for the test set of the average prediction CTR is: ', log_loss(df_val['click'], np.array([np.mean(df_val['click'])]*len(df_val['click']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "409ac7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = df_test['click']\n",
    "x_test = df_test.drop(columns=df_test.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59ff8304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss for the test set of Logistic Regression is:  0.44027694985590893\n"
     ]
    }
   ],
   "source": [
    "print('The loss for the test set of Logistic Regression is: ',logistic_regression.predict_loss(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6625192c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss for the test set of Naive Bayes is:  0.6733661971845487\n"
     ]
    }
   ],
   "source": [
    "print('The loss for the test set of Naive Bayes is: ',log_loss(y_test,naive_bayes.predict_proba(x_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
